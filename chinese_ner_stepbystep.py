from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import torch.nn.functional as F

# 1. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
model_name = "uer/roberta-base-finetuned-cluener2020-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
model.eval()

# 2. è¾“å…¥æ–‡æœ¬
text = "å°ç±³å…¬å¸ä½äºåŒ—äº¬ï¼Œé›·å†›æ˜¯åˆ›å§‹äººä¹‹ä¸€ã€‚"

# 3. åˆ†è¯ä¸ç¼–ç 
inputs = tokenizer(text, return_tensors="pt")
print("\nğŸ”¹ åˆ†è¯ç»“æœ tokenizer.tokenize():")
print(tokenizer.tokenize(text))

print("\nğŸ”¹ ç¼–ç ç»“æœ tokenizer():")
print(inputs)

# 4. æ¨¡å‹å‰å‘ä¼ æ’­
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

print("\nğŸ”¹ æ¨¡å‹è¾“å‡º logits:")
print(logits.shape)

# 5. å–æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
predictions = torch.argmax(logits, dim=-1)
print("\nğŸ”¹ é¢„æµ‹çš„æ ‡ç­¾ID:")
print(predictions)

# 6. æ˜ å°„æ ‡ç­¾ ID åˆ°å®é™…æ ‡ç­¾ï¼ˆBIOï¼‰
id2label = model.config.id2label
print("\nğŸ”¹ æ ‡ç­¾æ˜ å°„è¡¨:")
print(id2label)

labels = [id2label[label_id.item()] for label_id in predictions[0]]
print("\nğŸ”¹ æ ‡ç­¾æ˜ å°„ç»“æœï¼ˆBIOï¼‰:")
for token, label in zip(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0]), labels):
    print(f"{token}: {label}")

# 7. å®ä½“æå–ï¼šåˆå¹¶ B- å’Œ I- æ ‡ç­¾
def extract_entities(tokens, labels):
    entities = []
    current_entity = None
    current_type = None

    for token, label in zip(tokens, labels):
        if label.startswith("B-"):
            if current_entity:
                entities.append((current_entity, current_type))
            current_entity = token
            current_type = label[2:]
        elif label.startswith("I-") and current_entity:
            current_entity += token
        else:
            if current_entity:
                entities.append((current_entity, current_type))
                current_entity = None
                current_type = None
    if current_entity:
        entities.append((current_entity, current_type))
    return entities

tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
entities = extract_entities(tokens, labels)

print("\nğŸ”¹ å®ä½“è¯†åˆ«ç»“æœ:")
for word, entity_type in entities:
    print(f"å®ä½“: {word}ï¼Œç±»å‹: {entity_type}")